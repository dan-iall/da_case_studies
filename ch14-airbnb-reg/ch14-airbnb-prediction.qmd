---
title: Prepared for Gabor's Data Analysis
jupyter: python3
---


### Data Analysis for Business, Economics, and Policy
by Gabor Bekes and  Gabor Kezdi
 
Cambridge University Press 2021

**[gabors-data-analysis.com ](https://gabors-data-analysis.com/)**

 License: Free to share, modify and use for educational purposes. 
 Not to be used for commercial purposes.

### Chapter 14
**CH14B Predicting AirBnB apartment prices: selecting a regression model**

using the airbnb dataset

version 0.9.0 2025-08-14


```{python}
import os
import sys
import warnings

import numpy as np
import matplotlib as mpl
import patsy
import pandas as pd
import pyfixest as pf
import seaborn as sns
import matplotlib.pyplot as plt

warnings.filterwarnings("ignore")
```

```{python}
# Current script folder
current_path = os.getcwd()
dirname = current_path.split("da_case_studies")[0]
# location folders
data_in = dirname + "da_data_repo/airbnb/clean/"
data_out = dirname + "da_case_studies/ch14-airbnb-reg/"
output = dirname + "da_case_studies/ch14-airbnb-reg/output/"
func = dirname + "da_case_studies/ch00-tech-prep/"
sys.path.append(func)


import py_helper_functions as da

sns.set_theme(rc=da.da_theme, palette=da.color)
```

### Part 1

```{python}
# !!! make sure you have run ch14_airbnb_prepare.ipynb before
data = pd.read_csv(data_out + "airbnb_hackney_workfile_adj.csv")
```

Quick look at data

```{python}
data.describe()
```

where do we have missing variables now?

```{python}
data.isnull().sum().loc[lambda x: x > 0]
```

What to do with missing values?

   1. drop if no target (already did)

```{python}
data = data.dropna(subset=["price"])
```

 2. imput when few, not that important

```{python}
data["n_bathrooms"] = data["n_bathrooms"].fillna(np.nanmedian(data["n_bathrooms"]))
data["n_beds"] = data["n_beds"].fillna(data["n_accommodates"])
data["f_bathroom"] = data["f_bathroom"].fillna(1)
data["f_minimum_nights"] = data["f_minimum_nights"].fillna(1)
data["f_number_of_reviews"] = data["f_number_of_reviews"].fillna(1)
data["ln_beds"] = data["ln_beds"].fillna(0)
```

```{python}
data["n_bathrooms"].describe()
```

 3. drop columns when many missing not imortant

```{python}
data = data.drop(["usd_cleaning_fee", "p_host_response_rate"], axis=1)
```

 where do we have missing variables now?

```{python}
data.isnull().sum().loc[lambda x: x > 0]
```

 4. Replace missing variables re reviews with zero, when no review + add flags

```{python}
for var in ["flag_days_since", "flag_review_scores_rating", "flag_reviews_per_month"]:
    data[var] = [int(x) for x in data[var.replace("flag", "n")].isna()]
```

```{python}
data["n_days_since"] = data["n_days_since"].fillna(np.nanmedian(data["n_days_since"]))
data["n_review_scores_rating"] = data["n_review_scores_rating"].fillna(
    np.nanmedian(data["n_review_scores_rating"])
)
data["n_reviews_per_month"] = data["n_reviews_per_month"].fillna(
    np.nanmedian(data["n_reviews_per_month"])
)
```

```{python}
data.flag_days_since.value_counts()
```

Add features -> different functional forms

Create variables, measuring the time since: squared, cubic, logs

```{python}
data = data.assign(
    ln_days_since=lambda x: np.log(x["n_days_since"] + 1),
    ln_days_since2=lambda x: np.log(x["n_days_since"] + 1) ** 2,
    ln_days_since3=lambda x: np.log(x["n_days_since"] + 1) ** 3,
    n_days_since2=lambda x: x["n_days_since"] ** 2,
    n_days_since3=lambda x: x["n_days_since"] ** 3,
    ln_review_scores_rating=lambda x: np.log(x["n_review_scores_rating"]),
).assign(
    ln_days_since=lambda x: np.where(
        x["ln_days_since"].isnull(), 0, x["ln_days_since"]
    ),
    ln_days_since2=lambda x: np.where(
        x["ln_days_since2"].isnull(), 0, x["ln_days_since2"]
    ),
    ln_days_since3=lambda x: np.where(
        x["ln_days_since3"].isnull(), 0, x["ln_days_since3"]
    ),
)
```

Look at data

```{python}
data.describe()
```

 where do we have missing variables now?

```{python}
data.isnull().sum().loc[lambda x: x > 0]
```

### Business logic - define our prediction problem

Decision

Size, we need a normal apartment, 1-7persons

```{python}
data = data.loc[data.n_accommodates < 8]
```

```{python}
data.to_csv("airbnb_hackney_work.csv", index=False)
```

### Look at some descriptive statistics

How is the average price changing in my district by `property_type`, `room_type` and the `bed_type`?

```{python}
data.groupby(["f_property_type", "f_room_type"]).agg(mean_price=("price", np.mean))
```

```{python}
data.groupby(["f_bed_type"]).agg(mean_price=("price", np.mean))
```

```{python}
data.price.describe()
```

 NB all graphs, we exclude  extreme values of price

```{python}
datau = data.loc[data.price < 400]
```

Histograms price

Distribution of price by type below 400

### Figure 14.3 Airbnb apartment price and ln price distributions.

(a) Price

```{python}
sns.histplot(datau, x="price", bins=30, alpha=1, stat="probability")
plt.ylabel("Percent")
plt.xlabel("Price (US dollars)")
plt.yticks(
    np.arange(0, 0.16, 0.03),
    labels=[f"{100 * y:.0f}%" for y in np.arange(0, 0.16, 0.03)],
)
plt.show()
```

(b) Price in log

```{python}
sns.histplot(datau, x="ln_price", bins=30, alpha=1, stat="probability", binwidth=0.2)
plt.ylabel("Percent")
plt.xlabel("Price (US dollars)")
plt.yticks(
    np.arange(0, 0.16, 0.03),
    labels=[f"{100 * y:.0f}%" for y in np.arange(0, 0.16, 0.03)],
)
plt.show()
```

### Figure 14.4 Airbnb apartment price distribution by important features

(a) Price by room type

```{python}
datau["f_room_type"] = pd.Categorical(
    datau["f_room_type"], categories=sorted(set(datau["f_room_type"]), reverse=False)
)
```

```{python}
fig, ax = plt.subplots()
sns.boxplot(data=datau, x="f_room_type", y="price", width=0.8, ax=ax, showfliers=False)

box_line_col = [da.color[1], da.color[0], da.color[2]]
for i, box_col in enumerate(box_line_col):
    mybox = ax.patches[i]
    mybox.set_facecolor(mpl.colors.to_rgba(box_col, 0.3))
    mybox.set_edgecolor(box_col)
    for j in range(i * 5, i * 5 + 5):
        line = ax.lines[j]
        line.set_color(box_col)

plt.ylabel("Price (US dollars)", size=12)
plt.xlabel("Room type", size=12)
plt.yticks(da.seq(0, 300, 100))
da.add_margin(ax, x=0.1, y=0.01)
plt.show()
```

(b) Price by number of people accommodated and apartment versus house in US dollars

```{python}
datau["n_accommodates"] = datau["n_accommodates"].astype(int).astype("category")
```

```{python}
fig, ax = plt.subplots()
# Create the boxplot
sns.boxplot(
    x="n_accommodates",
    y="price",
    hue="f_property_type",
    data=datau,
    showfliers=False,
    palette={"Apartment": da.color[1], "House": da.color[0]},
)


for patch in ax.patches:
    r, g, b, _ = patch.get_facecolor()
    patch.set_facecolor((r, g, b, 0.7))
    patch.set_edgecolor((r, g, b, 0.7))


for i in range(0, int(len(ax.lines) / 2)):
    ax.lines[i].set_color(da.color[1])

for i in range(int(len(ax.lines) / 2), len(ax.lines)):
    ax.lines[i].set_color(da.color[0])

ax.legend(bbox_to_anchor=(0.4, 0.85), framealpha=0)
leg = ax.get_legend()
leg.set_title(None)
leg.legend_handles[0].set_color(da.color[1])
leg.legend_handles[1].set_color(da.color[0])

plt.ylabel("Price (US dollars)", size=12)
plt.xlabel("Guests accommodated", size=12)
plt.yticks(da.seq(0, 400, 50))
da.add_margin(ax, x=0.01, y=0.01)
plt.show()
```

### Part2

```{python}
data = pd.read_csv("airbnb_hackney_work.csv")
```

### Setting up models

Basic variables

```{python}
basic_lev = (
    "n_accommodates",
    "n_beds",
    "f_property_type",
    "f_room_type",
    "n_days_since",
    "flag_days_since",
)
basic_add = ("f_bathroom", "f_cancellation_policy", "f_bed_type")
reviews = ("f_number_of_reviews", "n_review_scores_rating", "flag_review_scores_rating")
poly_lev = ("n_accommodates2", "n_days_since2", "n_days_since3")
# not use p_host_response_rate due to missing obs
amenities = tuple(list(data.filter(regex="^d_.*")))
```

### Look for interactions

```{python}
plot_config = [
    {
        "x": "f_room_type",
        "y": "price",
        "hue": "d_familykidfriendly",
        "legend_title": "Family Kid Friendly",
    },
    {
        "x": "f_room_type",
        "y": "price",
        "hue": "f_property_type",
        "legend_title": "Property Type",
    },
    {
        "x": "f_cancellation_policy",
        "y": "price",
        "hue": "d_familykidfriendly",
        "legend_title": "Family Kid Friendly",
    },
    {"x": "f_cancellation_policy", "y": "price", "hue": "d_tv", "legend_title": "TV"},
    {"x": "f_property_type", "y": "price", "hue": "d_cats", "legend_title": "Cats"},
    {"x": "f_property_type", "y": "price", "hue": "d_dogs", "legend_title": "Dogs"},
]

da.plot_interactions(data, plot_config)
```

Dummies suggested by graphs

```{python}
X1 = ("f_room_type*f_property_type", "f_room_type*d_familykidfriendly")
```

 Additional interactions of factors and dummies

```{python}
X2 = (
    "d_airconditioning*f_property_type",
    "d_cats*f_property_type",
    "d_dogs*f_property_type",
)
X3 = (
    "(f_property_type + f_room_type + f_cancellation_policy + f_bed_type) * ("
    + "+".join(amenities)
    + ")"
)
```

Create estimation equations

```{python}
modellev1 = "~ n_accommodates"
modellev2 = "~" + " + ".join(basic_lev)
modellev3 = "~" + " + ".join(basic_lev + basic_add + reviews)
modellev4 = "~" + " + ".join(basic_lev + basic_add + reviews + poly_lev)
modellev5 = "~" + " + ".join(basic_lev + basic_add + reviews + poly_lev + X1)
modellev6 = "~" + " + ".join(basic_lev + basic_add + reviews + poly_lev + X1 + X2)
modellev7 = "~" + " + ".join(
    basic_lev + basic_add + reviews + poly_lev + X1 + X2 + amenities
)
modellev8 = (
    "~"
    + "+".join(basic_lev + basic_add + reviews + poly_lev + X1 + X2 + amenities)
    + "+"
    + X3
)

model_equations = [
    modellev1,
    modellev2,
    modellev3,
    modellev4,
    modellev5,
    modellev6,
    modellev7,
    modellev8,
]
```

### Create separate holdout set

```{python}
sample_size = round(0.2 * data.shape[0]) - 1
```

 Set the random number generator: It will make results reproducable

```{python}
np.random.seed(20180123)
```

```{python}
from sklearn.model_selection import train_test_split

data_work, data_holdout = train_test_split(data, test_size=sample_size)
```

### Crossvalidation

Use custom function `ols_crossvalidator` for cross validation

```{python}
# help(da.ols_crossvalidator)
```

Set number of folds

```{python}
n_folds = 5
```

Cross validate for each model and save results in `cv_list`

```{python}
cv_list = []
for model in model_equations:
    cv_list += [da.ols_crossvalidator("price" + model, data_work, n_folds)]
```

### Table 14.3 Comparing model fit measures

```{python}
compare_model_fits = (
    pd.DataFrame(cv_list)
    .round(2)
    .assign(
        Model=["M" + str(i + 1) for i in range(len(cv_list))],
        BIC=lambda x: x["BIC"].astype(int),
        Coefficients=lambda x: x["Coefficients"].astype(int),
    )
    .filter(["Model", "Coefficients", "R-squared", "BIC", "Training RMSE", "Test RMSE"])
)
compare_model_fits
```

### Figure 14.7 Training and test set RMSE for the eight regression models

```{python}
df_melted = compare_model_fits.melt(
    id_vars="Coefficients", value_vars=["Test RMSE", "Training RMSE"]
).assign(Coefficients=lambda x: x["Coefficients"].astype(str))

sns.lineplot(data=df_melted, x="Coefficients", y="value", hue="variable", linewidth=3)
plt.xlabel("Number of coefficients")
plt.ylabel("RMSE")
plt.ylim(30, 43)
plt.yticks(np.arange(30, 44, 2))
plt.legend(title="")
plt.show()
```

**NOTE:** Because the way we created the work and holdout sets relies on randomization, these results (eg. the number of coefficients) differ from R and book results. Even running the cross validation with exactly the same work set would result in slightly different RMSE results because we cannot control for the randomisation within the cross validation. Using exactly the same work set as R, the number of coefficients match though.

### Lasso

```{python}
from sklearn.linear_model import LassoCV
from sklearn.preprocessing import StandardScaler
```

Define Lasso equation from most complicated model

```{python}
vars_model_8 = modellev8
```

Define range for lambdas – the algo will look only in this set

```{python}
lambdas = np.arange(0.05, 1.01, 0.05)
print(lambdas)
```

To cross-validate lambda-s (which is Lasso's hyperparameter) one has to standardise the feature matrix.

```{python}
y, X = patsy.dmatrices("price" + vars_model_8, data_work)
X_featnames = X.design_info.column_names
scaler = StandardScaler()
X = scaler.fit_transform(X)
```

```{python}
lasso_fit = LassoCV(alphas=lambdas, cv=5, random_state=42).fit(X, y)
```

Best alpha the algorithm found

```{python}
lasso_fit.alpha_
```

```{python}
rmse_lambda_folds = (
    pd.DataFrame(lasso_fit.mse_path_, index=lambdas[::-1])
    .apply(np.sqrt)
    .mean(axis=1)
    .to_frame(name="Test RMSE")
    .round(2)
)
rmse_lambda_folds
```

Non-zero coefficients

```{python}
notnull_lasso_coefs = (
    pd.DataFrame(lasso_fit.coef_, index=X_featnames, columns=["coefficient"])
    .loc[lambda x: x["coefficient"].round(3) != 0]
    .round(3)
)
notnull_lasso_coefs
```

```{python}
compare_model_fits.loc[8, :] = {
    "Model": "Lasso",
    "Coefficients": notnull_lasso_coefs.shape[0],
    "Test RMSE": rmse_lambda_folds.loc[lasso_fit.alpha_].round(2).values[0],
}

compare_model_fits
```

The overall RMSE across the five test sets for the LASSO regression is 36.3. It is 36.49 for M7,
suggesting that our hand-picked M7 is almost as good as the LASSO regression. Our decision is to have the model M7 as our preferred
model, because it may be easier to explain which variables are included.

### Part3

#### Model diagnostics

```{python}
model7 = pf.feols("price" + modellev7, data=data_work)
```

Save the predicted values of model 7 ton data_holdout

```{python}
data_holdout["predicted_price"] = model7.predict(data_holdout)
```

Look at holdout RMSE of M7

```{python}
da.rmse(data_holdout["predicted_price"], data_holdout["price"])
```

work set RMSE refitted on the whole work set

```{python}
da.rmse(model7.predict(data_work), data_work["price"])
```

### Fitted vs actual outcome

### Figure 14.8 Airbnb prediction – model diagnostics

(a) Predicted price vs. actual price

```{python}
sns.scatterplot(data=data_holdout, x="predicted_price", y="price", s=20, alpha=0.7)
plt.plot([0, 350], [0, 350], linestyle="dashed", linewidth=2, color=da.color[1])
plt.xlim(0, 350)
plt.ylim(0, 350)
plt.xticks(np.arange(0, 351, 50))
plt.yticks(np.arange(0, 351, 50))
plt.xlabel("Predicted price (US dollars)")
plt.ylabel("Price (US dollars)")
plt.show()
```

Redo predicted values at 80% PI

```{python}
prediction_agg_by_nacc = (
    model7.predict(data_holdout, interval="prediction", alpha=0.2)
    .rename(columns={"fit": "predicted_price"})
    .assign(n_accommodates=data_holdout["n_accommodates"].values)
    .groupby(by=["n_accommodates"])
    .mean()
    .reset_index()
)
```

(b) Prediction interval by apartment size

```{python}
yci_low = prediction_agg_by_nacc["predicted_price"] - prediction_agg_by_nacc["ci_low"]
yci_upper = (
    prediction_agg_by_nacc["ci_high"] - prediction_agg_by_nacc["predicted_price"]
)

plt.bar(
    prediction_agg_by_nacc["n_accommodates"],
    prediction_agg_by_nacc["predicted_price"],
    yerr=[yci_low, yci_upper],
    capsize=4,
    color=da.color[0],
    alpha=0.9,
    error_kw={"elinewidth": 1, "ecolor": da.color[1]},
)
```

Density chart (not in book)

```{python}
sns.kdeplot(
    data=datau, x="price", hue="f_room_type", fill=True, alpha=0.3, common_norm=False
)
plt.xlabel("Price (US dollars)")
plt.ylabel("Density")
plt.show()
```

Barchart  (not in book)

```{python}
datau["n_accommodates"] = datau["n_accommodates"].astype(int)


sns.histplot(
    x="n_accommodates", hue="f_room_type", data=datau, alpha=0.8, multiple="stack"
)

plt.xlabel("Accomodates (Persons)")
plt.ylabel("Frequency")
plt.show()
```


